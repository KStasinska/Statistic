library(ggplot2)
beta1_values <- seq(-2, 2, by = 0.1)
data2 <- data.frame(beta1 = beta1_values)
calculate_power <- function(beta1) {
delta <- beta1 / sqrt(s2/SSX)
power <- pt(kwantyl, n - 2, ncp = delta) + (1 - pt(kwantyl, n - 2, ncp = delta))
return(power)
}
data2$power <- sapply(data2$beta1, calculate_power)
ggplot(data, aes(x = beta1, y = power)) +
geom_line() +
labs(title = "Moc testu w zależności od beta1",
x = "Wartość beta1",
y = "Moc testu") +
theme_minimal()
ggplot(data2, aes(x = beta1, y = power)) +
geom_line() +
labs(title = "Moc testu w zależności od beta1",
x = "Wartość beta1",
y = "Moc testu") +
theme_minimal()
beta1 = 1
n = 40
s2 = 70
SSX = 500
s2b1 = s2/SSX
delta = beta1 / sqrt(s2/SSX)
kwantyl=qt(0.975,n-2)
power=pt(-kwantyl, n-2, ncp = delta) + 1 - pt(kwantyl, n-2, ncp = delta)
power
library(ggplot2)
beta1_values <- seq(-2, 2, by = 0.1)
data2 <- data.frame(beta1 = beta1_values)
calculate_power <- function(beta1) {
delta <- beta1 / sqrt(s2/SSX)
power <- pt(-kwantyl, n - 2, ncp = delta) + 1 - pt(kwantyl, n - 2, ncp = delta)
return(power)
}
data2$power <- sapply(data2$beta1, calculate_power)
ggplot(data2, aes(x = beta1, y = power)) +
geom_line() +
labs(title = "Moc testu w zależności od beta1",
x = "Wartość beta1",
y = "Moc testu") +
theme_minimal()
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
library(MASS)
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
set.seed(123)  # Ustawienie ziarna dla powtarzalności
# Funkcja do przeprowadzania eksperymentu
run_experiment <- function(n, beta1, distribution) {
X <- matrix(rnorm(n * 200, mean = 0, sd = sqrt(500)), ncol = 200)
# Generowanie Y w zależności od wybranej dystrybucji szumu
if (distribution == "normal") {
epsilon <- matrix(rnorm(n, mean = 0, sd = 1), ncol = 1)
} else if (distribution == "exponential") {
epsilon <- matrix(rexp(n, rate = 1), ncol = 1)
} else if (distribution == "logistic") {
epsilon <- matrix(rlogis(n, location = 0, scale = 1), ncol = 1)
}
Y <- 5 + beta1 * X + epsilon
# Testowanie hipotezy H0: beta1 = 0
p_value <- summary(lm(Y ~ X))$coefficients[2, 4]
return(p_value < 0.05)  # Zwraca TRUE, jeśli odrzucamy H0
}
# Przeprowadzenie eksperymentów dla różnych warunków
conditions <- c("normal", "exponential", "logistic")
beta_values <- c(0, 2)
num_simulations <- 1000
results <- matrix(NA, nrow = length(conditions), ncol = length(beta_values))
for (i in seq_along(conditions)) {
for (j in seq_along(beta_values)) {
condition <- conditions[i]
beta_value <- beta_values[j]
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(100, beta_value, condition))
# Estymacja prawdopodobieństwa odrzucenia H0
results[i, j] <- mean(simulations)
}
}
for (i in seq_along(conditions)) {
for (j in seq_along(beta_values)) {
condition <- conditions[i]
beta_value <- beta_values[j]
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
# Estymacja prawdopodobieństwa odrzucenia H0
results[i, j] <- mean(simulations)
}
}
library(MASS)
library(MASS)
set.seed(123)
library(MASS)
set.seed(123)  # Ustawienie ziarna dla powtarzalności
# Funkcja do przeprowadzania eksperymentu
run_experiment <- function(n, beta1, distribution) {
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
# Generowanie Y w zależności od wybranej dystrybucji szumu
if (distribution == "normal") {
epsilon <- matrix(rnorm(n, mean = 0, sd = 1), ncol = 1)
} else if (distribution == "exponential") {
epsilon <- matrix(rexp(n, rate = 1), ncol = 1)
} else if (distribution == "logistic") {
epsilon <- matrix(rlogis(n, location = 0, scale = 1), ncol = 1)
}
Y <- 5 + beta1 * X + epsilon
# Testowanie hipotezy H0: beta1 = 0
p_value <- summary(lm(Y ~ X))$coefficients[2, 4]
return(p_value < 0.05)  # Zwraca TRUE, jeśli odrzucamy H0
}
# Przeprowadzenie eksperymentów dla różnych warunków
conditions <- c("normal", "exponential", "logistic")
beta_values <- c(0, 2)
num_simulations <- 1000
results <- matrix(NA, nrow = length(conditions), ncol = length(beta_values))
for (i in seq_along(conditions)) {
for (j in seq_along(beta_values)) {
condition <- conditions[i]
beta_value <- beta_values[j]
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
# Estymacja prawdopodobieństwa odrzucenia H0
results[i, j] <- mean(simulations)
}
}
# Wyświetlenie wyników
rownames(results) <- conditions
colnames(results) <- paste("Beta =", beta_values)
print(results)
View(results)
View(results)
View(results)
set.seed(123)  # Ustawienie ziarna dla powtarzalności
# Funkcja do przeprowadzania eksperymentu
run_experiment <- function(n, beta1, distribution) {
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
# Generowanie Y w zależności od wybranej dystrybucji szumu
if (distribution == "normal") {
epsilon <- rnorm(n, mean = 0, sd = 1)
} else if (distribution == "exponential") {
epsilon <- rexp(n, rate = 1)
} else if (distribution == "logistic") {
epsilon <- rlogis(n, location = 0, scale = 1)
}
Y <- 5 + beta1 * X + epsilon
# Testowanie hipotezy H0: beta1 = 0
p_value <- summary(lm(Y ~ X))$coefficients[2, 4]
return(p_value < 0.05)  # Zwraca TRUE, jeśli odrzucamy H0
}
# Przeprowadzenie eksperymentów dla różnych warunków
conditions <- c("normal", "exponential", "logistic")
beta_values <- c(0, 2)
num_simulations <- 1000
results <- matrix(NA, nrow = length(conditions), ncol = length(beta_values))
for (i in seq_along(conditions)) {
for (j in seq_along(beta_values)) {
condition <- conditions[i]
beta_value <- beta_values[j]
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
# Estymacja prawdopodobieństwa odrzucenia H0
results[i, j] <- mean(simulations)
}
}
condition <- conditions[i]
beta_value <- beta_values[j]
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
results[i, j] <- mean(simulations)
library(MASS)
set.seed(123)  # Ustawienie ziarna dla powtarzalności
# Funkcja do przeprowadzania eksperymentu
run_experiment <- function(n, beta1, distribution) {
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
# Generowanie Y w zależności od wybranej dystrybucji szumu
if (distribution == "normal") {
epsilon <- rnorm(n, mean = 0, sd = 1)
} else if (distribution == "exponential") {
epsilon <- rexp(n, rate = 1)
} else if (distribution == "logistic") {
epsilon <- rlogis(n, location = 0, scale = 1)
}
Y <- 5 + beta1 * X + epsilon
# Testowanie hipotezy H0: beta1 = 0
p_value <- summary(lm(Y ~ X))$coefficients[2, 4]
return(p_value < 0.05)  # Zwraca TRUE, jeśli odrzucamy H0
}
source("~/.active-rstudio-document", echo=TRUE)
library(MASS)
set.seed(123)  # Ustawienie ziarna dla powtarzalności
# Funkcja do przeprowadzania eksperymentu
run_experiment <- function(n, beta1, distribution) {
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
# Generowanie Y w zależności od wybranej dystrybucji szumu
if (distribution == "normal") {
epsilon <- rnorm(n, mean = 0, sd = 1)
} else if (distribution == "exponential") {
epsilon <- rexp(n, rate = 1)
} else if (distribution == "logistic") {
epsilon <- rlogis(n, location = 0, scale = 1)
}
Y <- 5 + beta1 * X + epsilon
# Testowanie hipotezy H0: beta1 = 0
p_value <- summary(lm(Y ~ X))$coefficients[2, 4]
return(p_value < 0.05)  # Zwraca TRUE, jeśli odrzucamy H0
}
# Przeprowadzenie eksperymentów dla różnych warunków
conditions <- c("normal", "exponential", "logistic")
beta_values <- c(0, 2)
num_simulations <- 1000
results <- matrix(NA, nrow = length(conditions), ncol = length(beta_values))
for (i in seq_along(conditions)) {
for (j in seq_along(beta_values)) {
condition <- conditions[i]
beta_value <- beta_values[j]
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
results[i, j] <- mean(simulations)
}
}
# Wyświetlenie wyników
rownames(results) <- conditions
colnames(results) <- paste("Beta =", beta_values)
print(results)
```{r,echo=FALSE}
path='/home/kasia/Desktop/Modele/tabela1_6.txt'
dane=read.table(path,col.names=c("indeks","GPA","IQ","płeć","PH"))
plot(GPA~IQ,dane)
reg1=lm(GPA~IQ, dane)
intercept=reg1$coefficients[1]
slope=reg1$coefficients[2]
abline(intercept,slope,col='green')
r_squared <- summary(reg1)$r.squared
print(paste("Współczynnik determinacji polecenia wbudowane: ", r_squared))
y_pred <- slope * dane$IQ + intercept
y_mean <- mean(dane$GPA)
ssm <- sum((y_pred - y_mean)^2)
sst <- sum((dane$GPA - y_mean)^2)
r_squared2=ssm/sst
print(paste("Współczynnik determinacji wzory teoretyczne: ", r_squared2))
msM <- sum((y_pred - y_mean)^2)/1
mse <- sum((dane$GPA-y_pred)^2)/(len(dane$GPA)-2)
msm <- sum((y_pred - y_mean)^2)/1
mse <- sum((dane$GPA-y_pred)^2)/(length(dane$GPA)-2)
f_statictic <- MSM/MSE
f_statistic <- summary(reg1)$fstatistic[1]
f_statictic <- msm/mse
p_value <- pf(summary(reg1)$fstatistic[1], summary(reg1)$fstatistic[2], summary(reg1)$fstatistic[3], lower.tail = FALSE)
p_value <- 1 - pf(f_statistic,76,length(dane$size)-2))
p_value <- 1 - pf(f_statistic,76,length(dane$size)-2)
p_value <- 1 - pf(f_statistic, 1, 76)
6/78
0.9*78
0.1*78
6/78
path='/home/kasia/Desktop/Modele/CH06PR15.txt'
dane=read.table(path,col.names=c("wiek","ciężkość","niepokój","satysfakcja"))
View(dane)
LinModR=lm(dane[,4] ~.,)
path='/home/kasia/Desktop/Modele/CH06PR15.txt'
dane=read.table(path,col.names=c("wiek","ciężkość","niepokój","satysfakcja"))
path='/home/kasia/Desktop/Modele/CH06PR15.txt'
dane=read.table(path,col.names=c("wiek","ciężkość","niepokój","satysfakcja"))
# Zadanie 1
## a)
```{r,echo=FALSE}
model = lm(dane[,4] ~., dane[,1:3])
model = lm(dane[,4] ~., dane[,1:3])
Polecenia wbudowane:
```{r,echo=FALSE}
cat("Y=", summary(model)$coefficients[1], "X1 +", summary(model)$coefficients[2],"X2 +", summary(model)$coefficients[3],"X3")
cat("Współczynnik R^2", summary(model)$r.squared)
cat("Y=", summary(model)$coefficients[1], "X1 +", summary(model)$coefficients[2],"X2 +", summary(model)$coefficients[3],"X3")
cat("Współczynnik R^2", summary(model)$r.squared)
Wzory teoretyczne:
```{r,echo=TRUE}
X = as.matrix(dane[,1:3])
Y = as.matrix(dane[,4])
Bety = solve(t(X) %*% X) %*% (t(X)) %*% Y
cat("Y=", Bety[1], "X1 +", Bety[2],"X2 +", Bety[3],"X3")
SSM = sum((predict(model) - mean(Y))^2)
SST = sum((Y - mean(Y))^2)
R2 = SSM/SST
cat("Współczynnik R^2", R2)
X = as.matrix(dane[,1:3])
Y = as.matrix(dane[,4])
Bety = solve(t(X) %*% X) %*% (t(X)) %*% Y
cat("Y=", Bety[1], "X1 +", Bety[2],"X2 +", Bety[3],"X3")
SSM = sum((predict(model) - mean(Y))^2)
SST = sum((Y - mean(Y))^2)
R2 = SSM/SST
cat("Współczynnik R^2", R2)
## b)
Rozważmy hipotezę $H_{0} : \beta_{1} = \beta_{2} = \beta_{3} = 0$ przeciwko $H_{1} : \beta_{1} \neq \, \vee \, \beta_{2} \neq 0 \, \vee \, \beta_{3} \neq 0$.
F - Statystyka testowa z rozkładu Fishera-Snedecora z 3 i 46 - 4 = 42 stopniami swobody.
```{r, echo=FALSE}
dfM = 3
dfE = 42
SSE = SST - SSM
MSE = SSE/dfE
MSM = SSM/dfM
F = MSM/MSE
criticF = qF(0.95,3,42)
criticF = qf(0.95,3,42)
criticF = qf(0.95,3,42)
pval = 1 - df(F)
pval = 1 - df(F,3,42)
summary(model)
df(F,3,42)
df(16.54,3,42)
pval = 1 - pf(F,3,42)
summary(model)$fstatistic[3]
summary(model)
summary(model)$p-value
summary(model)$`p-value`
summary(model)$`p-value`[1]
View(model)
summary = summary(model)
View(summary)
modelR = lm(dane[,4] ~., dane[,2:3])
anova(modelR,model)
summary=summary(anova(modelR,model))
modelR = lm(dane[,4] ~., dane[,2:3])
View(modelR)
summary(modelR)
model = lm(dane[,4] ~., dane[,1:3])
model
X = as.matrix(dane[,1:3])
View(X)
modelR = lm(dane[,4] ~., dane[,2:3])
modelR
lm(dane[,4] ~dane[,2]+dane[,3])
path='/home/kasia/Desktop/Modele/csdata.txt'
dane=read.table(path,col.names=c("id","GPA","HSM","HSS","HSE","SATM","SATV","SEX"))
View(dane)
model1 = lm(dane[,2] ~., dane[,3:5])
model2 = lm(dane[,2] ~., dane[,3:7])
View(model1)
View(model2)
anova(model1)
a = anova(model1)
View(a)
Y = dane[,2]
SST = sum((Y - mean(Y))^2)
SSM1 = sum((predict(model1) - mean(Y))^2)
SSM2 = sum((predict(model1) - mean(Y))^2)
Y = dane[,2]
SST = sum((Y - mean(Y))^2)
SSM1 = sum((predict(model1) - mean(Y))^2)
SSM2 = sum((predict(model2) - mean(Y))^2)
SSE1 = SST - SSM1
SSE2 = SST - SSM2
anova(model1)
F = (SSE1-SS2)/2/MSE
```{r,echo=FALSE}
model1 = lm(dane[,2] ~., dane[,3:5])
model2 = lm(dane[,2] ~., dane[,3:7])
Y = dane[,2]
SST = sum((Y - mean(Y))^2)
SSM1 = sum((predict(model1) - mean(Y))^2)
SSM2 = sum((predict(model2) - mean(Y))^2)
SSE1 = SST - SSM1
SSE2 = SST - SSM2
MSE = SSE/218
F = (SSE1-SSE2)/2/MSE
anova(model1,model2)
MSE = SSE2/218
F = (SSE1-SSE2)/2/MSE
anova(model1,model2)
model = lm(dane[,2] ~ dane[,6] + dane[,7] + dane[,3] + dane[,5] + dane[,4])
anova(model)
An=Anova(model)
library("car")
An=Anova(model)
install.packages("car")
library(car)
library('car')
library("car")
installed.packages('car')
installed.packages()
x = t(rstandard(model))
y = t(rstudent(model))
View(x)
plot(x)
plot(t(x))
x = rstandard(model)
hist(x)
View(X)
x = rstandard(model)
y = rstudent(model)
dffits <- as.data.frame(dffits(model))
View(dffits)
View(model)
5/224
sqrt(5/224)
2*sqrt(5/224)
View(dffits)
cookd(model)
library(car)
cookd(model)
cook.distance(model)
cooks.distance(model)
c = cooks.distance(model)
View(model)
sqrt(224)
2/sqrt(224)
dfbetas = as.data.frame(dfbetas(model))
View(dfbetas)
library(ggplot2)
library(ggplot2)
library(caret)
install.packages('caret')
library(caret)
dfbetas = as.data.frame(dfbetas(model))
ggplot(dfbetas, aes(x = seq_along(dfbetas[,1]), color = factor(rep(1:6, each = nrow(dfbetas))))) +
geom_point(aes(y = dfbetas[,1]), alpha = 0.7, position = position_jitter(width = 0.1)) +
geom_point(aes(y = dfbetas[,2]), alpha = 0.7, position = position_jitter(width = 0.1)) +
geom_point(aes(y = dfbetas[,3]), alpha = 0.7, position = position_jitter(width = 0.1)) +
geom_point(aes(y = dfbetas[,4]), alpha = 0.7, position = position_jitter(width = 0.1)) +
geom_point(aes(y = dfbetas[,5]), alpha = 0.7, position = position_jitter(width = 0.1)) +
geom_point(aes(y = dfbetas[,6]), alpha = 0.7, position = position_jitter(width = 0.1)) +
labs(color = 'k', x = 'Indeks obserwacji')
vif(model)
tolerance = 1/vif(model)
vif_values <- car::vif(modelz6)
dane2 = cbind(dane, SAT = dane[,6] + dane[,7])
modelz6 = lm(dane2[,2] ~ dane2[,6] + dane2[,7] + dane2[,9])
tolerance = 1/vif(modelz6)
vif_values <- car::vif(modelz6)
alias(modelz6)
vif_values <- car::vif(modelz6)
View(modelz6)
modelz6
model_step <- step(model, direction = "both", trace = 0)
View(model_step)
summary(model_step)
model_step <- step(model, direction = "both", trace = 0, k = log(224))
summary(model_step)
model_step_cp <- step(model_full, direction = "both", trace = 0, criteria = "Cp")
model_step_cp <- step(model, direction = "both", trace = 0, criteria = "Cp")
View(model_step_cp)
model_step <- step(model, direction = "both", trace = 0, criteria = "BIC")
summary(model_step)
library('olsrr')
install.packages("olsrr")
ols_step_both_adj_r2(model)
library(olsrr)
ols_step_both_adj_r2(model)
ols_step_both_aic(model)
ols_step_both_adj_r2(model)
ols_step_both_bic(model)
model_step_best_adj_r2 <- step(model, direction = "both", trace = 0, k = log(nrow(dane)), criterion = "adjr2")
View(model_step_best_adj_r2)
View(model_step_cp)
View(model_step_best_adj_r2)
View(model_step)
View(model_step_best_adj_r2)
View(model_step)
View(model_step_cp)
View(model_step)
model_step = step(model, direction = "both", trace = 0, criteria = "cp")
View(model_step)
help(step)
model_step = step(model, direction = "both", trace = 0, criteria = "bladkalanf")
View(model_step)
submodels <- regsubsets(model, method = "exhaustive", nvmax = ncol(data)-1)
submodels <- regsubsets(model, method = "exhaustive", nvmax = ncol(data)-1)
install.packages("leaps")
library(leaps)
submodels <- regsubsets(model, method = "exhaustive", nvmax = ncol(data)-1)
submodels <- regsubsets(y = data[, 2], x = data[, 3:8], method = "exhaustive", nvmax = ncol(data)-1)
subset_data <- data.frame(dane[, 2], data[, 3:8])
subset_data <- data.frame(dane[, 2], data[, 3:8])
submodels <- regsubsets(y = dane[, 2], x = dane[, 3:8], method = "exhaustive", nvmax = ncol(data)-1)
best_submodel <- which.min(submodels$cp)
submodels <- regsubsets(y = dane[, 2], x = dane[, 3:8], method = "exhaustive", nvmax = ncol(data)-1)
View(submodels)
submodels
submodels$cp
submodels$aic
summary(submodels)
submodels$cp
summary(subsets)
summary(submodels)
summary.submodels$cp
submodels <- summary.regsubsets(y = dane[, 2], x = dane[, 3:8], method = "exhaustive", nvmax = ncol(data)-1)
submodels <- summary.regsubsets(y = dane[, 2], x = dane[, 3:8])
regsubsets(model)
bestSubsets <- regsubsets(dane[,2] ~ dane[,3:8])
bestSubsets <- regsubsets(dane[,2] ~., dane[,3:8])
View(bestSubsets)
summary(bestSubsets)$adjr2
best_submodel <- which.min(summary(bestSubsets)$cp)
View(bestSubsets)
print(summary_subsets$which[best_submodel, ])
print(summary(bestSubsets)$which[best_submodel, ])
summary(bestSubsets)$which[best_submodel, ]
