leftslope=slope-kwantyl*s2b1
rightslope=slope+kwantyl*s2b1
kwantyl=qt(0.975, length(dane$hours)-2)
s2=sum((dane$hours-intercept-slope*dane$size)^2)/(length(dane$hours)-2)
s2b1=s2/sum((dane$size-mean(dane$size))^2)
leftslope=slope-kwantyl*s2b1^(1/2)
rightslope=slope+kwantyl*s2b1^(1/2)
s2b0=s2*(1/length(dane$size)+(mean(dane$size))^2/sum((dane$size-mean(dane$size))^2))
leftintercept=intercept-kwantyl*s2b0^(1/2)
rightintercept=intercept+kwantyl*s2b0^(1/2)
summary(reg1)
View(reg1)
pintercept=reg1t$coefficients["(Intercept)", "Pr(>|t|)"]
pintercept=reg1$coefficients["(Intercept)", "Pr(>|t|)"]
pintercept=summary(reg1)$coefficients["(Intercept)", "Pr(>|t|)"]
pslope=summary(reg1)$coefficients["(Intercept)", "Pr(>|t|)"]
pslope=summary(reg1)$coefficients["slope", "Pr(>|t|)"]
pslope=summary(reg1)$coefficients["size", "Pr(>|t|)"]
tintercept=summary(reg1)$coefficients["(Intercept)", "t value"]
tslope=summary(reg1)$coefficients["size", "t value"]
tslope=slope/(s2b1)^(1/2)
pslope=pt(abs(tslope),length(dane$size)-2)
pslope=summary(reg1)$coefficients["size", "Pr(>|t|)"]
pslope=pt(abs(tslope),length(dane$size)-2)
pslope=1-pt(abs(tslope),length(dane$size)-2)
pslope
print(pslope)
tslope1=slope/(s2b1)^(1/2)
pintercept=summary(reg1)$coefficients["(Intercept)", "Pr(>|t|)"]
pslope=summary(reg1)$coefficients["size", "Pr(>|t|)"]
tintercept=summary(reg1)$coefficients["(Intercept)", "t value"]
tslope=summary(reg1)$coefficients["size", "t value"]
tslope=slope/(s2b1)^(1/2)
slope=sum((dane$size-mean(dane$size))*(dane$hours-mean(dane$hours)))/sum((dane$size-mean(dane$size))^2)
intercept=mean(dane$hours-slope*mean(dane$size))
slope=sum((dane$size-mean(dane$size))*(dane$hours-mean(dane$hours)))/sum((dane$size-mean(dane$size))^2)
intercept=mean(dane$hours-slope*mean(dane$size))
```{r,echo=FALSE}
cat("Korzystając ze wzorów z wykładu intercept = ",intercept,", a slope = ",slope)
kwantyl=qt(0.975, length(dane$hours)-2)
s2=sum((dane$hours-intercept-slope*dane$size)^2)/(length(dane$hours)-2)
s2b1=s2/sum((dane$size-mean(dane$size))^2)
leftslope=slope-kwantyl*s2b1^(1/2)
rightslope=slope+kwantyl*s2b1^(1/2)
s2b0=s2*(1/length(dane$size)+(mean(dane$size))^2/sum((dane$size-mean(dane$size))^2))
leftintercept=intercept-kwantyl*s2b0^(1/2)
rightintercept=intercept+kwantyl*s2b0^(1/2)
pintercept=summary(reg1)$coefficients["(Intercept)", "Pr(>|t|)"]
pslope=summary(reg1)$coefficients["size", "Pr(>|t|)"]
tintercept=summary(reg1)$coefficients["(Intercept)", "t value"]
tslope=summary(reg1)$coefficients["size", "t value"]
tslope=slope/(s2b1)^(1/2)
pslope=1-pt(abs(tslope),length(dane$size)-2)
pslope=-pt(abs(tslope),length(dane$size)-2)
pslope=summary(reg1)$coefficients["size", "Pr(>|t|)"]
pslope=1-pt(abs(tslope),length(dane$size)-2)
knitr::opts_chunk$set(echo = TRUE)
t=qt(0.975, 18)
3-t
3+t
(t^2*16+9)^(1/2)
t
(t^2*16+9)^(1/2)
8.923115+16
-8.923115+16
knitr::opts_chunk$set(echo = TRUE)
for (k in c(1, 5, 8, 11, 25, 100)){
mu=intercept+slope*k
s2=(1/length(dane$size)+(k-mean(dane$size))^2/(sum(dane$size-mean(dane$size)^2)))
left=mu-kwantyl*s2
right=mu+kwantyl*s2
cat(paste("Przedział ufności dla k=",k,"[",left,",",right,"]"))
}
for (k in c(1, 5, 8, 11, 25, 100)){
mu=intercept+slope*k
s2=(1/length(dane$size)+(k-mean(dane$size))^2/(sum(dane$size-mean(dane$size)^2)))
left=mu-kwantyl*s2
right=mu+kwantyl*s2
cat(paste("Przedział ufności dla k=",k,"[",left,",",right,"] długość=",right-left,"\n"))
}
for (k in c(1, 5, 8, 11, 25, 100)){
mu=intercept+slope*k
s2=(1/length(dane$size)+(k-mean(dane$size))^2/(sum(dane$size-mean(dane$size)^2)))
left=mu-kwantyl*s2
right=mu+kwantyl*s2
cat(paste("Przedział ufności dla k=",k,"[",left,",",right,"]\n"))
cat(paste("długość=",right-left,"\n"))
}
mean(dane$size)
for (k in c(1, 5, 8, 11, 25,60,76,80, 100)){
mu=intercept+slope*k
s2=(1/length(dane$size)+(k-mean(dane$size))^2/(sum(dane$size-mean(dane$size)^2)))
left=mu-kwantyl*s2
right=mu+kwantyl*s2
cat(paste("Przedział ufności dla k=",k,"[",left,",",right,"]\n"))
cat(paste("długość=",right-left,"\n"))
}
for (k in c(1, 5, 8, 11, 25,60,76,80, 100)){
mu=intercept+slope*k
s2=(1/length(dane$size)+(k-mean(dane$size))^2/(sum(dane$size-mean(dane$size)^2)))
left=mu-kwantyl*s2^(1/2)
right=mu+kwantyl*s2^(1/2)
cat(paste("Przedział ufności dla k=",k,"[",left,",",right,"]\n"))
cat(paste("długość=",right-left,"\n"))
}
(k-mean(dane$size))^2/(sum(dane$size-mean(dane$size)^2)))
(k-mean(dane$size))^2/(sum(dane$size-mean(dane$size)^2))
(k-mean(dane$size))^2
s2=sum((dane$hours-intercept-slope*dane$size)^2)/(length(dane$hours)-2)
(sum(dane$size-mean(dane$size)^2))
for (k in c(1, 5, 8, 11, 25,60,76,80, 100)){
mu=intercept+slope*k
smu2=s2*(1/length(dane$size)+(k-mean(dane$size))^2/(sum((dane$size-mean(dane$size))^2)))
left=mu-kwantyl*s2^(1/2)
right=mu+kwantyl*s2^(1/2)
cat(paste("Przedział ufności dla k=",k,"[",left,",",right,"]\n"))
cat(paste("długość=",right-left,"\n"))
}
for (k in c(1, 5, 8, 11, 25,60,76,80, 100)){
mu=intercept+slope*k
smu2=s2*(1/length(dane$size)+(k-mean(dane$size))^2/(sum((dane$size-mean(dane$size))^2)))
left=mu-kwantyl*smu2^(1/2)
right=mu+kwantyl*smu2^(1/2)
cat(paste("Przedział ufności dla k=",k,"[",left,",",right,"]\n"))
cat(paste("długość=",right-left,"\n"))
}
for (k in c(1, 5, 8, 11, 25,60,76,80, 100)){
mu=intercept+slope*k
smu2=s2*(1/length(dane$size)+(k-mean(dane$size))^2/(sum((dane$size-mean(dane$size))^2)))
left=mu-kwantyl*smu2^(1/2)
right=mu+kwantyl*smu2^(1/2)
cat(paste("Wartość estymatora wartości oczekiwanej czasu obsługi:",mu))
cat(paste("Przedział ufności dla k=",k,"[",left,",",right,"]\n"))
cat(paste("długość=",right-left,"\n"))
}
for (k in c(1, 5, 8, 11, 25,60,76,80, 100)){
mu=intercept+slope*k
smu2=s2*(1/length(dane$size)+(k-mean(dane$size))^2/(sum((dane$size-mean(dane$size))^2)))
left=mu-kwantyl*smu2^(1/2)
right=mu+kwantyl*smu2^(1/2)
cat(paste("Wartość estymatora wartości oczekiwanej czasu obsługi:",mu,"\n"))
cat(paste("Przedział ufności dla k=",k,"[",left,",",right,"]\n"))
cat(paste("długość=",right-left,"\n"))
}
# Wartości k, dla których chcesz obliczyć przedziały ufności
k_values <- c(1, 5, 8, 11, 25, 60, 76, 80, 100)
# Obliczenia
for (k in k_values) {
new_data <- data.frame(size = k)  # Nowe dane do przewidzenia
pred <- predict(reg1, newdata = new_data, interval = "confidence", level = 0.95)
cat(paste("Wartość estymatora wartości oczekiwanej czasu obsługi:", pred[1], "\n"))
cat(paste("Przedział ufności dla k=", k, "[", pred[2], ",", pred[3], "]\n"))
cat(paste("Długość przedziału:", pred[3] - pred[2], "\n\n"))
}
mean(dane$size)
for (k in c(1, 5, 8, 11, 25, 60, 76, 77, 80, 100)){
mu=intercept+slope*k
spred2=s2*(1+1/length(dane$size)+(k-mean(dane$size))^2/(sum((dane$size-mean(dane$size))^2)))
left=mu-kwantyl*spred2^(1/2)
right=mu+kwantyl*spred2^(1/2)
cat(paste("Przewidywany czas obsługi:",mu,"\n"))
cat(paste("Przedział ufności dla k=",k,"[",left,",",right,"]\n"))
cat(paste("Długość przedziału=",right-left,"\n\n"))
}
View(reg1)
View(dane)
new_data <- data.frame(size = k)
View(new_data)
new_data<-data.frame(size=c(1,2))
View(new_data)
new_data <- data.frame(size = c(1, 5, 8, 11, 25, 100))
conf <- predict(model, newdata = new_data, interval = "confidence", level = 0.95)
conf <- predict(reg1, newdata = new_data, interval = "confidence", level = 0.95)
pred <- predict(reg1, newdata = new_data, interval = "prediction", level = 0.95)
View(conf)
View(conf)
library(ggplot2)
ggplot(df, aes(x = size)) +
geom_ribbon(aes(ymin = conf_lwr, ymax = conf_upr), fill = "blue", alpha = 0.2, linetype = "solid", color = "blue", size = 1) +
geom_ribbon(aes(ymin = pred_lwr, ymax = pred_upr), fill = "red", alpha = 0.2, linetype = "dashed", color = "red", size = 1) +
labs(title = "Przedziały ufności (niebieskie) i predykcyjne (czerwone)") +
theme_minimal()
View(conf)
# Dodawanie przedziałów ufności do wykresu
lines(dane$size, conf$fit, col = "blue", lty = 1)
# Dodawanie przedziałów ufności do wykresu
lines(dane$size, conf[, "fit"], col = "blue", lty = 1)
# Dodawanie przedziałów ufności do wykresu
lines(c(1,5,8,11,25,100), conf[, "fit"], col = "blue", lty = 1)
plot(hours~size, dane)
abline(intercept,slope,col='green')
# Dodawanie przedziałów ufności do wykresu
lines(c(1,5,8,11,25,100), conf[, "fit"], col = "blue", lty = 1)
plot.new()
plot(hours~size, dane)
abline(intercept,slope,col='green')
plot.new()
plot(hours~size, dane)
# Dodawanie przedziałów ufności do wykresu
lines(c(1,5,8,11,25,100), conf[, "fit"], col = "blue", lty = 1)
abline(reg1,col='green')
plot.new()
plot(hours~size, dane)
abline(reg1,col='green')
View(dane)
cat(kwantyl=qt(0.975, 18))
n=40
s2=70
SSX=500
s2b1=s2/SSX
delta=1/s2b1^(1/2)
power=pt(kwantyl, n-2, delta) + (1 - pt(kwantyl, n-2,delta))
n=40
s2=70
SSX=500
s2b1=s2/SSX
delta=1/s2b1^(1/2)
kwantyl=qt(0.975,n-2)
power=pt(kwantyl, n-2, delta) + (1 - pt(kwantyl, n-2,delta))
pt(kwantyl, n-2, delta)
(1 - pt(kwantyl, n-2,delta))
beta1_values <- seq(-2, 2, by = 0.1)
data2 <- data.frame(beta1 = beta1_values)
calculate_power <- function(beta1) {
delta <- beta1 / sqrt(s2/SSX)
power <- pt(kwantyl, n - 2, ncp = delta) + (1 - pt(kwantyl, n - 2, ncp = delta))
return(power)
}
data2$power <- sapply(data2$beta1, calculate_power)
ggplot(data, aes(x = beta1, y = power)) +
geom_line() +
labs(title = "Moc testu w zależności od beta1",
x = "Wartość beta1",
y = "Moc testu") +
theme_minimal()
library(ggplot2)
beta1_values <- seq(-2, 2, by = 0.1)
data2 <- data.frame(beta1 = beta1_values)
calculate_power <- function(beta1) {
delta <- beta1 / sqrt(s2/SSX)
power <- pt(kwantyl, n - 2, ncp = delta) + (1 - pt(kwantyl, n - 2, ncp = delta))
return(power)
}
data2$power <- sapply(data2$beta1, calculate_power)
ggplot(data, aes(x = beta1, y = power)) +
geom_line() +
labs(title = "Moc testu w zależności od beta1",
x = "Wartość beta1",
y = "Moc testu") +
theme_minimal()
ggplot(data2, aes(x = beta1, y = power)) +
geom_line() +
labs(title = "Moc testu w zależności od beta1",
x = "Wartość beta1",
y = "Moc testu") +
theme_minimal()
beta1 = 1
n = 40
s2 = 70
SSX = 500
s2b1 = s2/SSX
delta = beta1 / sqrt(s2/SSX)
kwantyl=qt(0.975,n-2)
power=pt(-kwantyl, n-2, ncp = delta) + 1 - pt(kwantyl, n-2, ncp = delta)
power
library(ggplot2)
beta1_values <- seq(-2, 2, by = 0.1)
data2 <- data.frame(beta1 = beta1_values)
calculate_power <- function(beta1) {
delta <- beta1 / sqrt(s2/SSX)
power <- pt(-kwantyl, n - 2, ncp = delta) + 1 - pt(kwantyl, n - 2, ncp = delta)
return(power)
}
data2$power <- sapply(data2$beta1, calculate_power)
ggplot(data2, aes(x = beta1, y = power)) +
geom_line() +
labs(title = "Moc testu w zależności od beta1",
x = "Wartość beta1",
y = "Moc testu") +
theme_minimal()
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
library(MASS)
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
set.seed(123)  # Ustawienie ziarna dla powtarzalności
# Funkcja do przeprowadzania eksperymentu
run_experiment <- function(n, beta1, distribution) {
X <- matrix(rnorm(n * 200, mean = 0, sd = sqrt(500)), ncol = 200)
# Generowanie Y w zależności od wybranej dystrybucji szumu
if (distribution == "normal") {
epsilon <- matrix(rnorm(n, mean = 0, sd = 1), ncol = 1)
} else if (distribution == "exponential") {
epsilon <- matrix(rexp(n, rate = 1), ncol = 1)
} else if (distribution == "logistic") {
epsilon <- matrix(rlogis(n, location = 0, scale = 1), ncol = 1)
}
Y <- 5 + beta1 * X + epsilon
# Testowanie hipotezy H0: beta1 = 0
p_value <- summary(lm(Y ~ X))$coefficients[2, 4]
return(p_value < 0.05)  # Zwraca TRUE, jeśli odrzucamy H0
}
# Przeprowadzenie eksperymentów dla różnych warunków
conditions <- c("normal", "exponential", "logistic")
beta_values <- c(0, 2)
num_simulations <- 1000
results <- matrix(NA, nrow = length(conditions), ncol = length(beta_values))
for (i in seq_along(conditions)) {
for (j in seq_along(beta_values)) {
condition <- conditions[i]
beta_value <- beta_values[j]
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(100, beta_value, condition))
# Estymacja prawdopodobieństwa odrzucenia H0
results[i, j] <- mean(simulations)
}
}
for (i in seq_along(conditions)) {
for (j in seq_along(beta_values)) {
condition <- conditions[i]
beta_value <- beta_values[j]
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
# Estymacja prawdopodobieństwa odrzucenia H0
results[i, j] <- mean(simulations)
}
}
library(MASS)
library(MASS)
set.seed(123)
library(MASS)
set.seed(123)  # Ustawienie ziarna dla powtarzalności
# Funkcja do przeprowadzania eksperymentu
run_experiment <- function(n, beta1, distribution) {
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
# Generowanie Y w zależności od wybranej dystrybucji szumu
if (distribution == "normal") {
epsilon <- matrix(rnorm(n, mean = 0, sd = 1), ncol = 1)
} else if (distribution == "exponential") {
epsilon <- matrix(rexp(n, rate = 1), ncol = 1)
} else if (distribution == "logistic") {
epsilon <- matrix(rlogis(n, location = 0, scale = 1), ncol = 1)
}
Y <- 5 + beta1 * X + epsilon
# Testowanie hipotezy H0: beta1 = 0
p_value <- summary(lm(Y ~ X))$coefficients[2, 4]
return(p_value < 0.05)  # Zwraca TRUE, jeśli odrzucamy H0
}
# Przeprowadzenie eksperymentów dla różnych warunków
conditions <- c("normal", "exponential", "logistic")
beta_values <- c(0, 2)
num_simulations <- 1000
results <- matrix(NA, nrow = length(conditions), ncol = length(beta_values))
for (i in seq_along(conditions)) {
for (j in seq_along(beta_values)) {
condition <- conditions[i]
beta_value <- beta_values[j]
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
# Estymacja prawdopodobieństwa odrzucenia H0
results[i, j] <- mean(simulations)
}
}
# Wyświetlenie wyników
rownames(results) <- conditions
colnames(results) <- paste("Beta =", beta_values)
print(results)
View(results)
View(results)
View(results)
set.seed(123)  # Ustawienie ziarna dla powtarzalności
# Funkcja do przeprowadzania eksperymentu
run_experiment <- function(n, beta1, distribution) {
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
# Generowanie Y w zależności od wybranej dystrybucji szumu
if (distribution == "normal") {
epsilon <- rnorm(n, mean = 0, sd = 1)
} else if (distribution == "exponential") {
epsilon <- rexp(n, rate = 1)
} else if (distribution == "logistic") {
epsilon <- rlogis(n, location = 0, scale = 1)
}
Y <- 5 + beta1 * X + epsilon
# Testowanie hipotezy H0: beta1 = 0
p_value <- summary(lm(Y ~ X))$coefficients[2, 4]
return(p_value < 0.05)  # Zwraca TRUE, jeśli odrzucamy H0
}
# Przeprowadzenie eksperymentów dla różnych warunków
conditions <- c("normal", "exponential", "logistic")
beta_values <- c(0, 2)
num_simulations <- 1000
results <- matrix(NA, nrow = length(conditions), ncol = length(beta_values))
for (i in seq_along(conditions)) {
for (j in seq_along(beta_values)) {
condition <- conditions[i]
beta_value <- beta_values[j]
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
# Estymacja prawdopodobieństwa odrzucenia H0
results[i, j] <- mean(simulations)
}
}
condition <- conditions[i]
beta_value <- beta_values[j]
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
# Przeprowadzenie wielu symulacji
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
results[i, j] <- mean(simulations)
library(MASS)
set.seed(123)  # Ustawienie ziarna dla powtarzalności
# Funkcja do przeprowadzania eksperymentu
run_experiment <- function(n, beta1, distribution) {
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
# Generowanie Y w zależności od wybranej dystrybucji szumu
if (distribution == "normal") {
epsilon <- rnorm(n, mean = 0, sd = 1)
} else if (distribution == "exponential") {
epsilon <- rexp(n, rate = 1)
} else if (distribution == "logistic") {
epsilon <- rlogis(n, location = 0, scale = 1)
}
Y <- 5 + beta1 * X + epsilon
# Testowanie hipotezy H0: beta1 = 0
p_value <- summary(lm(Y ~ X))$coefficients[2, 4]
return(p_value < 0.05)  # Zwraca TRUE, jeśli odrzucamy H0
}
source("~/.active-rstudio-document", echo=TRUE)
library(MASS)
set.seed(123)  # Ustawienie ziarna dla powtarzalności
# Funkcja do przeprowadzania eksperymentu
run_experiment <- function(n, beta1, distribution) {
mean = rep(c(0),200)
var = diag(200)/500
X = mvrnorm(1, mean,var)
# Generowanie Y w zależności od wybranej dystrybucji szumu
if (distribution == "normal") {
epsilon <- rnorm(n, mean = 0, sd = 1)
} else if (distribution == "exponential") {
epsilon <- rexp(n, rate = 1)
} else if (distribution == "logistic") {
epsilon <- rlogis(n, location = 0, scale = 1)
}
Y <- 5 + beta1 * X + epsilon
# Testowanie hipotezy H0: beta1 = 0
p_value <- summary(lm(Y ~ X))$coefficients[2, 4]
return(p_value < 0.05)  # Zwraca TRUE, jeśli odrzucamy H0
}
# Przeprowadzenie eksperymentów dla różnych warunków
conditions <- c("normal", "exponential", "logistic")
beta_values <- c(0, 2)
num_simulations <- 1000
results <- matrix(NA, nrow = length(conditions), ncol = length(beta_values))
for (i in seq_along(conditions)) {
for (j in seq_along(beta_values)) {
condition <- conditions[i]
beta_value <- beta_values[j]
simulations <- replicate(num_simulations, run_experiment(200, beta_value, condition))
results[i, j] <- mean(simulations)
}
}
# Wyświetlenie wyników
rownames(results) <- conditions
colnames(results) <- paste("Beta =", beta_values)
print(results)
```{r,echo=FALSE}
path='/home/kasia/Desktop/Modele/tabela1_6.txt'
dane=read.table(path,col.names=c("indeks","GPA","IQ","płeć","PH"))
plot(GPA~IQ,dane)
reg1=lm(GPA~IQ, dane)
intercept=reg1$coefficients[1]
slope=reg1$coefficients[2]
abline(intercept,slope,col='green')
r_squared <- summary(reg1)$r.squared
print(paste("Współczynnik determinacji polecenia wbudowane: ", r_squared))
y_pred <- slope * dane$IQ + intercept
y_mean <- mean(dane$GPA)
ssm <- sum((y_pred - y_mean)^2)
sst <- sum((dane$GPA - y_mean)^2)
r_squared2=ssm/sst
print(paste("Współczynnik determinacji wzory teoretyczne: ", r_squared2))
msM <- sum((y_pred - y_mean)^2)/1
mse <- sum((dane$GPA-y_pred)^2)/(len(dane$GPA)-2)
msm <- sum((y_pred - y_mean)^2)/1
mse <- sum((dane$GPA-y_pred)^2)/(length(dane$GPA)-2)
f_statictic <- MSM/MSE
f_statistic <- summary(reg1)$fstatistic[1]
f_statictic <- msm/mse
p_value <- pf(summary(reg1)$fstatistic[1], summary(reg1)$fstatistic[2], summary(reg1)$fstatistic[3], lower.tail = FALSE)
p_value <- 1 - pf(f_statistic,76,length(dane$size)-2))
p_value <- 1 - pf(f_statistic,76,length(dane$size)-2)
p_value <- 1 - pf(f_statistic, 1, 76)
6/78
0.9*78
0.1*78
6/78
View(dane)
knitr::opts_chunk$set(echo = TRUE)
path='/home/kasia/Desktop/Modele/CH01PR20.txt'
dane=read.table(path,col.names=c("hours", "size"))
plot(hours~size,dane)
mean(size)
mean(dane$size)
